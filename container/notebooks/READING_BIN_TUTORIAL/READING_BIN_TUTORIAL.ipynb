{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Introduction\n",
    "In this tutorial, you will learn how to read L3 TRMM binary data into Python.  TRMM is the  [Tropical Rainfall Measuring Mission](https://trmm.gsfc.nasa.gov/).  As an aside, it is recommended to use netCDF-4 formatted data for Python instead of binary files.  However, this tutorial will take you through the process of extracting data from the binary files.\n",
    "\n",
    "# Import Dependencies\n",
    "Let's import all the libraries we need. This needs to be done before any of the other cells can be run. These libraries were installed in the docker container you are using, so we will not need to worry about installing anything. Simply running the following cell takes care of all of the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import netCDF4\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Approving the GES DISC DAAC\n",
    " \n",
    "Before we can obtain any GES DISC data, we need to approve the GES DISC DAAC in our Earthdata Account. If you do not yet have an Earthdata Login Account, [you can create one here](https://urs.earthdata.nasa.gov). Next, we will need to approve the GES DISC data. Go to [this link](https://urs.earthdata.nasa.gov/approve_app?client_id=e2WVk8Pw6weeLUKZYOxvTQ) and click approve. If you are prompted to login, fill out your login information and click the link again.\n",
    "\n",
    "# Obtaining Data\n",
    "\n",
    "We first need to obtain some TRMM L3 data. <br> <br> \n",
    "The TRMM L3 data repository is located here: ftp://trmmopen.gsfc.nasa.gov/pub/merged/3B42RT/ <br> \n",
    "<br>\n",
    "After this page fully loads (this might take a moment), you can enter a directory for a given year. From here, you can enter into a directory for each month.  For the purpose of this tutorial, we will use the second dataset for the year 2010. \n",
    "<br>\n",
    "<br>\n",
    "In this case, the full link to this dataset is: ftp://trmmopen.gsfc.nasa.gov/pub/merged/3B42RT/2010/01/3B42RT.2010010103.7R2.bin.gz\n",
    "<br>\n",
    "<br>\n",
    "If you go to the above website, it will automatically download this file to your local machine (not part of this tutorial).\n",
    "We will use the wget command to download this file to our local directory. Run the following command in your terminal (for information on how to use `wget`, see the `ACOS_DATA_HANDLING` tutorial).\n",
    "\n",
    "> `cd /home/condauser/tutorials/notebooks/READING_BIN_TUTORIAL`\n",
    "\n",
    "> `wget \"ftp://trmmopen.gsfc.nasa.gov/pub/merged/3B42RT/2010/01/3B42RT.2010010103.7R2.bin.gz\"`\n",
    "\n",
    "Now, there is a file (`3B42RT.2010010103.7R2.bin.gz`) in your local directory.  You can see this by running the following command:\n",
    "\n",
    "> `ls`\n",
    "\n",
    "Let's unpack this to the bin file by using the following commands in your terminal.\n",
    "\n",
    "> `gunzip 3B42RT.2010010103.7R2.bin.gz`\n",
    "\n",
    "These commands unzip the binary file. Now, we are ready to read the file into python. Run one more command to see the binary file in your local directory:\n",
    "\n",
    "> `ls`\n",
    "\n",
    "# Reading in the file to Python\n",
    "Open the binary file using the \"rb\" as the I/O mode. This will ensure that Python understand that this file is binary.  The header length for binary TRMM data is 2880 character.  Let's look at the header for this bin file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binAddress = '/home/condauser/tutorials/notebooks/READING_BIN_TUTORIAL/3B42RT.2010010103.7R2.bin'\n",
    "binFile = open(binAddress, 'rb')\n",
    "\n",
    "binData = binFile.read()\n",
    "headerLength = 2880\n",
    "\n",
    "header = [i for i in binData[0:headerLength].decode('utf-8').replace(',', ' ').split(' ') if i != \"\"]\n",
    "\n",
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Header Information\n",
    "Looking through the header, we can find a great deal of useful information that will be necessary for furthing unpacking the data:\n",
    "\n",
    "* The number of latitude bins is 480\n",
    "* The number of longitude bins is 1440\n",
    "* Latitude bounds: 60S to 60N\n",
    "* Longitude bounds: 0E to 360E\n",
    "* Grid resolution: 0.25x0.25 degrees Lat/Lon\n",
    "* Byte Order = Big Endian\n",
    "\n",
    "You can also obtain the number of bins by looking at the grid size and the range of coordinates. In this case, the longitude is bound by 0E to 360E.  Latitude is bound by 60S to 60N. Each grid bin is 0.25x0.25 degrees lat/lon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(360 / 0.25)\n",
    "print(120 / 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "* Variable Information:\n",
    "<pre>\n",
    "| Names | precipitation   | precipitation_error |        source       | uncal_precipitation |\n",
    "|-------|-----------------|---------------------|---------------------|---------------------|\n",
    "| Units |       mm/hr     |        mm/hr        |     source number   |        mm/hr        |\n",
    "|-------|-----------------|---------------------|---------------------|---------------------|\n",
    "| Scale |       100       |         100         |           1         |         100         |\n",
    "|-------|-----------------|---------------------|---------------------|---------------------|\n",
    "| Type  | signed_integer2 |    signed_integer2  |    signed_integer1  |    signed_integer2  |\n",
    "|-------|-----------------|---------------------|---------------------|---------------------|\n",
    "</pre>\n",
    "* file_byte_length=(char2880)_header+(int2)x1440x480x2_data+(int1)x1440x480_data+(int2)x1440x480_data\n",
    "\n",
    "We can see that most of the data is of type *signed_integer2* (2 bytes of information).\n",
    "\n",
    "Now, we can understand the file itself.  The first 2880 bytes are the header. The next 1382400 bytes are the precipitation data (440 x 1440 x 2).  The next 1382400 bytes are the precipitation error data.  The following 691200 bytes are the source data.  The last 1382400 bytes are the uncalibrated precipitation data.  In total, that is 4841280 bytes (or characters). Let's check to make sure that is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(binData) == 4841280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = 480\n",
    "cols = 1440\n",
    "dataLengthBytes = 2 * rows * cols + headerLength\n",
    "dataLengthBytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Gathering the Data\n",
    "Since we read the file as a binary file, we can use the `np.fromstring()` function to import our data. In the header, we saw that the variable type of the precipitation variable was `signed_integer2`, so we can tell our function to read the data from this format by putting in the `np.dtype(\">i2\")` function.  The `>` in front of `i2` reads the binary file as Big Endian (specified in the header).  We then convert this to a float and rescale our variable by dividing by 100 (as mentioned in the header). Finally, this is now just a big 1-Dimensional array of our data. But it should be gridded by latitude and longitude, as per the header. So we use the `reshape()` procedure to create this grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precipBin = np.fromstring(binData[headerLength:dataLengthBytes], np.dtype(\">i2\"))\n",
    "precipBin = np.asarray(precipBin, np.dtype(\"float32\"))\n",
    "precipBin /= 100\n",
    "precipBin = precipBin.reshape(rows,cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Cleaning the Data\n",
    "We need to mask all the values where the error flag appears (-31999, as per the header)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precipBinMasked = np.ma.masked_where(precipBin < -300, precipBin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Data Statistics\n",
    "Let's record some basic statistics about our imported data to compare with the `.nc4` version of the same data we have found online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binMean = precipBinMasked.mean()\n",
    "binMax = precipBinMasked.max()\n",
    "binMin = precipBinMasked.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading `.nc4` Data\n",
    "Now, let's download the `.nc4` version of our dataset to confirm that we have been converting our binary correctly.  The same data in .nc4 is stored at at [this OPeNDAP server](https://disc2.nascom.nasa.gov/opendap/hyrax/). You can:\n",
    "\n",
    "1. Open the directory: TRMM_RT/\n",
    "2. Open the directory: TRMM_3B42RT.7/\n",
    "3. Open the directory: 2010/\n",
    "4. Open the directory: 001/\n",
    "\n",
    "\n",
    "\n",
    "For this tutorial, however, we will use `wget`. For more information on the `wget` command, see `ACOS_DATA_HANDLING` notebook. \n",
    "\n",
    "> `cd /home/condauser/tutorials/notebooks/READING_BIN_TUTORIAL`\n",
    "\n",
    "Next, we need to input your username and password for Earthdata login. Run the following command and enter your username and password when prompted:\n",
    "\n",
    "> `read -p \"enter your username: \" username; read -s -p \"enter your password: \" password; echo \"\"`\n",
    "\n",
    "To download this data, run the following command in your terminal:\n",
    "\n",
    "> `cd /home/condauser/tutorials/notebooks/READING_BIN_TUTORIAL`\n",
    "\n",
    "> `wget --user=\"$username\" --password=\"$password\" \"https://disc2.nascom.nasa.gov/opendap/TRMM_RT/TRMM_3B42RT.7/2010/001/3B42RT.2010010103.7R2.nc4.nc4\"; username=\"\"; password=\"\"`\n",
    "\n",
    "Now, type in the following command to confirm that the .nc4 file was downloaded into your local directory\n",
    "\n",
    "> `ls`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NC4Address = '/home/condauser/tutorials/notebooks/READING_BIN_TUTORIAL/3B42RT.2010010103.7R2.nc4.nc4'\n",
    "\n",
    "\n",
    "NC4File = netCDF4.Dataset(NC4Address)\n",
    "\n",
    "latNC4 = NC4File[\"lat\"][:]\n",
    "lonNC4 = NC4File[\"lon\"][:]\n",
    "\n",
    "precipNC4 = NC4File[\"precipitation\"][:]\n",
    "\n",
    "latsNC4, lonsNC4 = np.meshgrid(latNC4, lonNC4)\n",
    "\n",
    "NC4Mean = precipNC4.mean()\n",
    "\n",
    "NC4Min = precipNC4.min()\n",
    "\n",
    "NC4Max = precipNC4.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Checking the Data is Equal\n",
    "Let's see if our `.nc4` data is the same as our `.bin` converted data. Since the Data is represented in slightly different ways (more on this later), we cannot simply compare the matrices. We will use sample statistics for a quick confirmation, and we will visually confirm using a plot later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Checking Equality.\")\n",
    "if abs(1 - binMean/NC4Mean) < 0.05:\n",
    "    print(\"Means equal.\")\n",
    "    \n",
    "if binMax == NC4Max:\n",
    "    print(\"Maxes equal.\")\n",
    "    \n",
    "if binMin == NC4Min:\n",
    "    print(\"Mins equal.\")\n",
    "\n",
    "print(\"Finished Checking Equality.\")\n",
    "\n",
    "def is_equal(binary_file, nc4_file):\n",
    "    bfin = open(binary_file, 'rb')\n",
    "    bdat = bfin.read()\n",
    "    bhdr = bdat[0:2880].decode('utf-8').strip()\n",
    "    import netCDF4\n",
    "    return bhdr in str(netCDF4.Dataset(nc4_file))\n",
    "\n",
    "is_equal(binAddress, NC4Address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Transposing the grd\n",
    "The precipitation data from the binary file has the following shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precipBinMasked.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "You will notice, however, that the number of longitude and latitude bins are 1440 and 480, respectively.  For plotting purposes, we are going to transpose our data in order to match the number of bins.  This is because the mapping package used later on in this tutorials reads data in (longitude, latitude) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precipBinMasked = precipBinMasked.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precipBinMasked.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Translating the grid\n",
    "The data we translated from binary is represented slightly differently than the `.nc4` data we found online. The `.bin` file is plotted from 0$^\\circ$ to 360$^\\circ$ longitude. The `.nc4` file is plotted from -180$^\\circ$ to 180$^\\circ$ longitude. To take care of this, we shift our measurements from the binary file over by 180 degrees. Our grid is at increments of 0.25$^\\circ$ longitude, so we need to shift our matrix over by 720 indices. We can do this using the `np.roll()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = np.roll(np.fliplr(precipBinMasked), 720, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Setting up the Plot\n",
    "Now that our data agrees, we can set up our plot by creating a mesh-grid of our latitude and longitude data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latStart = -60\n",
    "latFinish = 60\n",
    "latStep = 0.25\n",
    "\n",
    "lonStart = -180\n",
    "lonFinish = 180\n",
    "lonStep = 0.25\n",
    "\n",
    "latBin = np.arange(latStart,latFinish, latStep)\n",
    "lonBin = np.arange(lonStart, lonFinish, lonStep)\n",
    "\n",
    "latsBin, lonsBin = np.meshgrid(latBin, lonBin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Plotting the Data\n",
    "Here, we plot the data. For information on how to do this, see the `ACOS_DATA_HANDLING` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(211)\n",
    "b = Basemap(projection='cyl',llcrnrlat=-60,urcrnrlat=60,\\\n",
    "            llcrnrlon=-180,urcrnrlon=180,resolution='l')\n",
    "b.drawcoastlines()\n",
    "cs = b.contourf(lonsBin, latsBin, l, latlon=True)\n",
    "cbar = b.colorbar(cs, location='bottom', pad='5%')\n",
    "cbar.set_label(\"mm/hr\")\n",
    "plt.title(\"Map Generated with Binary File\")\n",
    "\n",
    "plt.subplot(212)\n",
    "b = Basemap(projection='cyl',llcrnrlat=-60,urcrnrlat=60,\\\n",
    "            llcrnrlon=-180,urcrnrlon=180,resolution='l')\n",
    "b.drawcoastlines()\n",
    "cs = b.contourf(lonsNC4, latsNC4, precipNC4.T, latlon=True)\n",
    "cbar = b.colorbar(cs, location='bottom', pad='5%')\n",
    "cbar.set_label(\"mm/hr\")\n",
    "plt.title(\"Map Generated with .nc4 File\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
