{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "We have started this page in an attempt to give users guidance on how to read ACOS data in python. We prepared relatively simple examples. To remind, ACOS data are in HDF5 format.\n",
    "\n",
    "The emphasis  here as given on reading and quality screening is done on purpose. These steps are the most important to understand by novice users. Once they become confident in opening files and reading data from there, they can augment examples here with the particular usage they have in mind.\n",
    "\n",
    "# Import Dependencies\n",
    "\n",
    "Let's import all the libraries we need. This needs to be done before any of the other cells can be run. These libraries were installed in the docker container you are using, so we will not need to worry about installing anything. Simply running the following cell takes care of all of the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from numpy.ma import masked_array\n",
    "%matplotlib inline\n",
    "import os\n",
    "import glob\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Data\n",
    "First, we need to get all the names of the URLs we will be downloading from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Data\n",
    "\n",
    "We will be downloading data from the [GESDISC data archive](https://oco2.gesdisc.eosdis.nasa.gov). You can feel free to explore the data available there. We will be downloading data from March, 2010. If you navigate to `GOSAT_TANSO_Level2/ACOS_L2S.7.3/2010` folder, you will see 365 folders to view (and download) the data from all of 2010. If you click on `/060`, you will see all the files from the 60th day of 2010 (March 1st). As you can see, there are a lot of files from each day. To make it easier to download all this data (instead of manually clicking on each of the links on each of the 31 days) we will use `wget` to download everyting in one simple command.\n",
    "\n",
    "We can use `wget` to download the names of the `.h5` files. This command will go to each of these URLs and download any files linked on that webpage ending in `.h5`. Before we can downlaod anything, let's create this URL file.\n",
    "\n",
    "### Creating URL file\n",
    "We first need to create a URL file with the URLs of all the files we want to download. We will write this out to `URLs.txt` in the `ACOS_DATA_HANDLING` folder. We can use python to do this. Before creating the URL file, run the following in your terminal.\n",
    "\n",
    "> `cd /home/condauser/tutorials/notebooks/ACOS_DATA_HANDLING`\n",
    "\n",
    "> `ls`\n",
    "\n",
    "As you can see, there is no URL file, just the jupyter notebook you are currently looking at. Looking at the folder naming scheme on the data link [here](https://oco2.gesdisc.eosdis.nasa.gov/data/GOSAT_TANSO_Level2/ACOS_L2S.7.3/2010/), you see that the URLs are named nicely, so we can just use a script to create our URL file.\n",
    "\n",
    "Now run the following cell here in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/condauser/tutorials/notebooks/ACOS_DATA_HANDLING\")\n",
    "URLs = []\n",
    "\n",
    "# dates corresponding to March 1 - 31, 2010\n",
    "start = 60\n",
    "end = 90\n",
    "for i in range(start, end + 1):\n",
    "    URLs.append(\"https://oco2.gesdisc.eosdis.nasa.gov/data/GOSAT_TANSO_Level2/ACOS_L2S.7.3/2010/0{}/\".format(\n",
    "        str(i).zfill(2)))\n",
    "open(\"URLs.txt\", \"w\").write(\"\\n\".join(URLs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a list of the URLs we are interested in getting data from. This is stored in `URLs.txt` in our current directory. Run `ls` again in your terminal and you will see that there is now a file called `URLs.txt`. You can inspect this file by running the following in your terminal.\n",
    "\n",
    "> `cd /home/condauser/tutorials/notebooks/ACOS_DATA_HANDLING`\n",
    "\n",
    "> `nano URLs.txt`\n",
    "\n",
    "As you can see, there are 31 URLs in this file, one per line. To exit this text editor, press `CTRL+X`. Now that we have the URLs we want to get the files from, we can proceed to the downloading step.\n",
    "\n",
    "## Download Data\n",
    "\n",
    "Let's create a folder to store all the new data. Before doing this, let's see what's in our `ACOS_DATA_HANDLING` folder. Run the following in your terminal to do this.\n",
    "\n",
    "> `cd /home/condauser/tutorials/notebooks/ACOS_DATA_HANDLING`\n",
    "\n",
    "> `ls`\n",
    "\n",
    "As of now, only the jupyter notebook and the `URLs.txt` file are in this folder. Now, let's create the new folder.\n",
    "\n",
    "> `cd /home/condauser/tutorials/notebooks/ACOS_DATA_HANDLING`\n",
    "\n",
    "> `mkdir ACOS_DATA`\n",
    "\n",
    "Now, run `ls` again in your terminal. As you can see, we successfully created a folder called `ACOS_DATA`. Let's check that this folder is empty. Run the following in your terminal.\n",
    "\n",
    "> `cd /home/condauser/tutorials/notebooks/ACOS_DATA_HANDLING/ACOS_DATA`\n",
    "\n",
    "> `ls`\n",
    "\n",
    "So you can see that this notebook is empty. Next, we will run a `wget` command to download all the data. Note that this command will take several minutes to run.\n",
    "\n",
    "> `cd /home/condauser/tutorials/notebooks/ACOS_DATA_HANDLING/ACOS_DATA`\n",
    "\n",
    "> `wget --user=<username> --password=<password> -r -c -nH -nd -np -A h5 -i ../URLs.txt`\n",
    "\n",
    "When this command finishes, you should see your command prompt (`condauser:/ACOS_DATA_HANDLING $`) at the bottom of your terminal. After this finishes, run the following command.\n",
    "\n",
    "> `history -c`\n",
    "\n",
    "The `history` command clears your command line history so your username and passwords are not stored there. After this command has finished (you will see the terminal prompt again), let's confirm that these files have downloaded. We can do this by running the following commands in your terminal.\n",
    "\n",
    "> `cd /home/condauser/tutorials/notebooks/ACOS_DATA_HANDLING/ACOS_DATA`\n",
    "\n",
    "> `ls -U | head -10`\n",
    "\n",
    "The second command here returns the first 10 items in this directory. From this, you will see that our download has been successful, and this folder now has the data we are interested in. Now, let's import this data into python.\n",
    "\n",
    "## Import Data\n",
    "\n",
    "We need to import all this data into this python environment, but first we need all the names of the files we just imported. We can use the `glob` package to find these names.\n",
    "\n",
    "### Getting File Names\n",
    "\n",
    "The `glob` function within the `glob` package gives all the names with the specified extension. First, make sure the current directory is `acos_data` (to see your current directory, you can run `os.getcwd()` in python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/condauser/tutorials/notebooks/ACOS_DATA_HANDLING/ACOS_DATA\")\n",
    "filenames = glob.glob(\"/home/condauser/tutorials/notebooks/ACOS_DATA_HANDLING/ACOS_DATA/*.h5\")\n",
    "len(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have successfully downloaded 455 `.h5` datasets!\n",
    "### File Contents\n",
    "\n",
    "Next, we need to import all the data from each of these files. Since we will be averageing across time, we do not need to worry about storing the data by time; we can simply put all the data in one place. When importing these datasets, the data is stored within \"groups\" and within each group is a series of variables. Let's import one dataset and look at all the groups and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = netCDF4.Dataset(filenames[0])\n",
    "for grp in data.groups:\n",
    "    print(grp)\n",
    "    print(\"-\" * 40)\n",
    "    for var in data.groups[grp].variables:\n",
    "        print(var)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a huge number of groups and variables stored in this dataset. For this tutorial, we will be using the variable `xco2`, which is in `RetrievalResults` group. We will also need the variables `sounding_latitude` and `sounding_longitude` from the `SoundingGeometry` group. \n",
    "\n",
    "### Pulling the Files into Python\n",
    "\n",
    "We can import each of these files in turn using the following function and script. Run the following cell. Note that it may take a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataImport(infile):\n",
    "    data = netCDF4.Dataset(infile)\n",
    "    xco2_vals = data.groups[\"RetrievalResults\"].variables[\"xco2\"][:]\n",
    "    lat_vals = data.groups[\"SoundingGeometry\"].variables[\"sounding_latitude\"][:]\n",
    "    lon_vals = data.groups[\"SoundingGeometry\"].variables[\"sounding_longitude\"][:]\n",
    "    return [xco2_vals, lat_vals, lon_vals]\n",
    "\n",
    "xco2 = []\n",
    "lat = []\n",
    "lon = []\n",
    "    \n",
    "for file in filenames:\n",
    "    out = dataImport(file)\n",
    "    xco2.extend(out[0])\n",
    "    lat.extend(out[1])\n",
    "    lon.extend(out[2])\n",
    "\n",
    "if len(xco2) == len(lat) == len(lon):\n",
    "    print(\"successfully imported {} datapoints\".format(len(xco2)))\n",
    "else:\n",
    "    print(\"something went wrong in importation...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a little over 50,000 observations. Now, we need to get them into the correct format to plot with `Basemap`. \n",
    "\n",
    "# Formatting the Data\n",
    "\n",
    "Currently, we have our data in triples of the form `(xco2, lat, lon)`, but we want our `xco2` data to be in a grid format, where each row corresponds to a value of latitude and each row corresponds to a value of longitude. But since  the latitude and longitude measurements given to us are fairly exact, we will round down our measurements of latitude and longitude to increments of 0.5. We will then average values of `xco2` within each value of latitude and longitude.\n",
    "\n",
    "## Gridding our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduceDimension(data, lats, lons, squareSize):\n",
    "    \n",
    "    transLat = np.arange(-90,90 + squareSize,squareSize)\n",
    "    transLon = np.arange(-180,180 + squareSize,squareSize)\n",
    "    \n",
    "    nLat = len(transLat)\n",
    "    nLon = len(transLon)\n",
    "    \n",
    "    transData = [[[] for j in range(nLon)] for i in range(nLat)] #np.zeros((nLat,nLon))\n",
    "    \n",
    "    transMask = np.full((nLat,nLon),False)\n",
    "    \n",
    "    # reassign points\n",
    "    for point,lat,lon in zip(data, lats, lons):\n",
    "        roundLatIndex = int(lat / squareSize + 90 / squareSize) #% len(transLat)\n",
    "        roundLonIndex = int(lon / squareSize + 180 / squareSize) #% len(transLon)\n",
    "        if lat >= -90.0:\n",
    "            transData[roundLatIndex][roundLonIndex].append(point)\n",
    "        \n",
    "    # could be multiple points per square, take mean\n",
    "    for i,row in enumerate(transData):\n",
    "        for j,square in enumerate(row):\n",
    "            if not square:\n",
    "                transData[i][j] = 0\n",
    "                transMask[i][j] = True\n",
    "            else:\n",
    "                # convert to PPM\n",
    "                transData[i][j] = 1000000 * np.mean(square)\n",
    "                \n",
    "    transDataMasked = masked_array(transData, transMask)\n",
    "    \n",
    "    return [transDataMasked, transLat, transLon]\n",
    "\n",
    "[trans_xco2_masked, transLat, transLon] = reduceDimension(xco2, lat, lon, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Data\n",
    "\n",
    "Now, onto the fun part. Since we have our data in the correct format, it is fairly easy to throw the information into `Basemap` to get a nice looking plot. The following function takes data, latitude and longitude and plots on a robinson projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data should be masked if necessary (it likely is necessary)\n",
    "def plotWorld(data, lat, lon):\n",
    "    [lons, lats] = np.meshgrid(lon, lat)\n",
    "    plt.figure(figsize = (15,15))\n",
    "    b = Basemap(projection=\"robin\", lat_0=0, lon_0=0)\n",
    "    b.drawcoastlines()\n",
    "    cs = b.pcolor(lons,lats, data, shading='flat', vmin=350, vmax=400, latlon=True)\n",
    "    cbar = b.colorbar(cs, location=\"bottom\", pad=\"5%\")\n",
    "    cbar.set_label(\"CO2 PPM\")\n",
    "    plt.title(\"CO2 PPM Days {} - {}\".format(start,end))\n",
    "\n",
    "plotWorld(trans_xco2_masked, transLat, transLon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is a simple way to download data using `wget`, import and format data using `netCDF4` and plot the data using `Basemap`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
